<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>mscthesis-paper-reading-preparation</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<h2 id="master-thesis-preparation">Master thesis preparation</h2>
<h3 id="topic-data-quality">Topic: data quality</h3>
<h4 id="automating-large-scale-data-quality-verification-link">1.
Automating Large-Scale Data Quality Verification <a
href="https://www.amazon.science/publications/automating-large-scale-data-quality-verification">link</a></h4>
<p>This article defines data quality in to following dimensions:</p>
<ul>
<li>completeness</li>
<li>consistency</li>
<li>accuracy</li>
</ul>
<p>The main approach provided in this article is doing ‚ÄúUnit Test‚Äù to
Data. This tool was then open sourced by Amazon teams, namely,
<strong><a
href="https://github.com/awslabs/deequ?tab=readme-ov-file">deequ</a></strong></p>
<p>This approach allow <strong>user</strong> to define a set of
<strong>constraint</strong> for data quality checks. Then the
<strong>computable metrics</strong> should also be defined which
provided a way to measure the current data quality regards to above
mentioned three dimension. After these information and the verified data
have been set up, the system inspects the checks and their constraints,
and collects the metrics required to evaluate the checks. The output of
the system will be a report showcase the qualification of the defined
metrics on dataset.</p>
<p>The later part of article also mention how to extend this approach
for <strong>incrementally</strong> growing datasets(in the case of
<strong>ingestion pipelines</strong> in a <strong>data
warehouse</strong>).</p>
<h4
id="data-quality-challenges-in-large-scale-cyber-physical-systems-link">2.
Data quality challenges in large-scale cyber-physical systems <a
href="https://www.sciencedirect.com/science/article/pii/S0306437921001484">Link</a>:</h4>
<p>This article focus on a data quality management solution to detect
errors in sensor nodes‚Äôs measurement in large scale CPS systems.</p>
<p>Cyber-physical systems (CPSs) are integrated systems engineered to
combine computational control algorithms and <strong>physical
components</strong> such as sensors and actuators, effectively using an
embedded communication core. <strong>Large-scale CPSs</strong> are
vulnerable to enormous technical and operational challenges that may
compromise the quality of data of their applications and accordingly
reduce the quality of their services.</p>
<p>This article defined following data quality dimensions:</p>
<ul>
<li>accuracy</li>
<li>timeliness</li>
<li>completeness</li>
<li>consistency</li>
</ul>
<p>This research use a methodology called SLR(systematic literature
review) to understand the data quality main challenges while categorize
them into above mentioned dimensions and proposed solution based on the
SLR results.</p>
<h3 id="topic-big-data-smart-manufacturing-data-lifecycle">Topic: Big
data, Smart manufacturing, data lifecycle</h3>
<h4
id="an-industrial-big-data-pipeline-for-datadriven-analytics-maintenance-applications-in-largescale-smart-manufacturing-facilities">3.
An industrial big data pipeline for data‚Äëdriven analytics maintenance
applications in large‚Äëscale smart manufacturing facilities</h4>
<p>This article provide a system design for an industrial big data
pipeline of large-scale smart manufacturing facilities(IoT/CPS for
example). Although it‚Äôs not directly related to data quality, but still
provide a through study of an information system model that provides a
scalable and fault tolerant big data pipeline for integrating,
processing and analyzing industrial equipment data.</p>
<h4 id="data-driven-smart-manufacturing-link">4. Data-driven smart
manufacturing <a
href="https://www.sciencedirect.com/science/article/pii/S0278612518300062?casa_token=m2ie8GSzdEMAAAAA:m8AOpj3JMrqMFJ5iJEq4ebZ00c829r6hGGbgc_cOBZI0fHJoAwV88GzfjWwxzIxNm2k5J5NHRA">link</a></h4>
<p>This article covered all the aspect regards to the entire Lifecycle
of manufacturing data. Could be used as a good start point and reference
to understand our current situation.</p>
<p>The lifecycle included:</p>
<ul>
<li>Data source</li>
<li>Data collection</li>
<li>Data storage</li>
<li>Data processing</li>
<li>Data visualization</li>
<li>Data transmission</li>
<li>Data application</li>
</ul>
<h3 id="topic-data-warehouse">Topic: data warehouse</h3>
<h4 id="snowflake-the-original-paper">5. snowflake the original
paper</h4>
<p>The main difference of snowflake to other traditional data warehouse
technology is it is design for the benefit for <strong>cloud</strong>.
It‚Äôs processing engine and most of other part are developed from scratch
instead of using existing big data technology like hadoop.</p>
<h5 id="snowflake-vs-shared-nothing-architectures">snowflake vs
shared-nothing architectures</h5>
<ul>
<li><p>Heterogeneous Workload</p>
<p>In share-nothing architectures, multiple nodes usually have same
hardware configuration. There could be two different type of tasks(one
is I/O intensive the other is CPU intensive) This need the hardware to
be configured in a trade-off with low average utilization. Platform like
Amazon EC2 allow different instance type from which this share-nothing
architectures cannot take advantage.</p></li>
<li><p>Membership changes</p>
<p>In the cloud concept, the node failures are more frequent and
performance can vary even in the same type of nodes(EC2 instances). This
could be a result of non-virtualized resources like network bandwidth.
When it happens, data need to be reshuffled between nodes which will
impact the system performance.</p></li>
</ul>
<p>Snowflake solution: separates storage and compute. Storage in Amazon
S3(blob store). Compute in snowflake shared-nothing engine.</p>
<p><strong>How SF shored data in S3</strong></p>
<p>‚ùå S3: No possible to append data the end of file instead of fully
overwritten.</p>
<p>üí° Snowflake: Tables are horizontally partitioned into immutable
files.</p>
<p>‚úÖ S3: Support GET requests for parts of a file.</p>
<p>üí° Snowflake: Within each file, the values of same column or
attribute are grouped together and heavily compressed. Each table file
has a header contain the offsets of each column within the file.</p>
<p><strong>Compute QUERY: Virtual Warehouse</strong></p>
<ul>
<li><p>Isolation</p>
<p>Each query runs on one VW. VM of different size can have different
number of worker nodes (a EC2 instance). Worker nodes not shared across
VWs resulting in strong <strong>performance isolation</strong> for
queries.</p></li>
<li><p>Consistent hashing: improve caching with query optimizer</p>
<p>LRU used to manage cache replacement in each worker node. Optimizer
assigns input file sets to worker nodes using consistent hashing over
table file names. The queries accessing to same table file will then be
put to the same worker node.</p></li>
<li><p>Execution engine</p>
<p>TODO..</p></li>
</ul>
<h3 id="topic-data-steam-processing">Topic: data steam processing</h3>
<h4 id="apache-flink-original-paper">6. apache-flink original paper</h4>
<p>P.S. Below is my personal understanding based on the paper, could be
inaccurate and would be adjust in the furture.</p>
<h5 id="problems-to-sovle">Problems to sovle</h5>
<ul>
<li>Batch(static) data stream processing ignore the time features.</li>
<li>‚Äúlambda architecture‚Äù combine batch and stream processing. The
stream layer is responsible for the calculation of some time related
features from data which changing in real-time and the batch layer is to
calculate the statistic features from a period of time which don‚Äôt need
to be real time. This archtecture suffers from the latency and
complexity.
<ul>
<li>latency: when the application need both data, they have to pull them
from these two different sources.</li>
<li>complexity: if the data schema changes, both places of code need to
be changed.</li>
<li>exists tradeoff between data freshness and accuracy</li>
</ul></li>
</ul>
<h5 id="main-contribution">Main contribution</h5>
<ul>
<li>a unified architecture for both stream and batch processing
with:</li>
<li>a windowing mechanism</li>
<li>a batch processer</li>
<li>that represent a streaming dataflow for streaming, batch, iterative,
and interactive analytics.</li>
</ul>
</body>
</html>
